<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic Manipulation">
  <meta name="keywords" content="imitation learning mobile manipulation, robots ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic Manipulation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <h1 style="font-size: 55px; font-weight: bold; text-align: center;">PIVOT-R: Primitive-Driven Waypoint-Aware World Model <br> for Robotic Manipulation</h1>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="is-size-5 publication-authors" style="padding-top: 10px;">
            <span class="author-block">
              Kaidong Zhang<sup>1</sup><sup>&dagger;</sup>,
            </span>
            <span class="author-block">
              Pengzhen Ren<sup>2</sup><sup>&dagger;</sup>,
            </span>
            <span class="author-block">
              Bingqian Lin<sup>1</sup>,
            </span>
            <span class="author-block">
              Junfan Lin<sup>2</sup>,
            </span>
            <span class="author-block">
              Shikui Ma<sup>3</sup>,
            </span>
            <span class="author-block">
              Hang Xu<sup>4</sup>,
            </span>
            <span class="author-block">
              Xiaodan Liang <sup>12</sup><sup>*</sup>
            </span>
          </div>
          <br>

          <div class="is-size-5 publication-authors">
            <span class="font-size: 20px;"><sup>1</sup>Sun Yat-Sen University,<sup>2</sup>Peng Cheng Laboratory,<sup>3</sup>Dataa Robotics,<sup>4</sup>Huawei Noah's Ark Lab</span>
            <br>
            <span style="font-size: 20px;"><sup>&dagger;</sup>Equal Contribution</span>
            <br>
            <span style="font-size: 20px;"><sup>*</sup>Corresponding author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv Link. -->
              <span class="">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span style="font-size: 22px;">arXiv</span>
                </a>
              </span>
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span style="font-size: 22px;">Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span style="font-size: 22px;">Code Coming Soon</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <!-- <div class="content has-text-justified">
    <div class="columns is-centered has-text-centered">
      <div class="container">
        <video poster="" id="calvin_0" autoplay controls muted loop playsinline width="24%" style="border-radius: 4%; position: relative; left: -1%;">
          <source src="static/videos/intro/calvin_0.mp4"
                  type="video/mp4">
        </video>
        <video poster="" id="calvin_1" autoplay controls muted loop playsinline width="24%" style="border-radius: 4%; ; position: relative; left: -0.5%;">
          <source src="static/videos/intro/calvin_1.mp4"
                  type="video/mp4">
        </video>
        <video poster="" id="calvin_2" autoplay controls muted loop playsinline width="24%" style="border-radius: 4%; position: relative; left: 0.5%;">
          <source src="static/videos/intro/calvin_2.mp4"
                  type="video/mp4">
        </video>
        <video poster="" id="calvin_3" autoplay controls muted loop playsinline width="24%" style="border-radius: 4%; ; position: relative; left: 1%;">
          <source src="static/videos/intro/calvin_3.mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
           <div class="item open_the_drawer">
            <video poster="" id="open_the_drawer" autoplay controls muted loop playsinline height="90%">
              <source src="static/videos/real_exp/open_the_drawer.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item 3task_distractor">
            <video poster="" id="3task_distractor" autoplay controls muted loop playsinline height="90%">
              <source src="static/videos/intro/3task_distractor.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item 3task_unseen">
            <video poster="" id="3task_unseen" autoplay controls muted loop playsinline height="90%">
              <source src="static/videos/intro/3task_unseen.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item 3task_normal">
            <video poster="" id="3task_normal" autoplay controls muted loop playsinline height="90%">
              <source src="static/videos/intro/3task_normal.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>

  </div> -->

  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            Language-guided robotic manipulation is a challenging task that requires an embodied agent to follow abstract user instructions to accomplish various complex manipulation tasks. 
            Previous work trivially fitting the data without revealing the relation between instruction and low-level executable actions, these models are prone to memorizing the surficial pattern of the data instead of acquiring the transferable knowledge, and thus are fragile to dynamic environment changes. 
            To address this issue, we propose a <b>P</b>r<b>I</b>rmitive-dri<b>V</b>en wayp<b>O</b>in<b>T</b>-aware world model for <b>R</b>obotic manipulation (<b>PIVOT-R</b>) that focuses solely on the prediction of task-relevant waypoints.
            Specifically, <b>PIVOT-R</b> consists of a Waypoint-aware World Model (WAWM) and a lightweight action prediction module. The former performs primitive action parsing and primitive-driven waypoint prediction, while the latter focuses on decoding low-level actions. Additionally, we also design an asynchronous hierarchical executor (AHE), which can use different execution frequencies for different modules of the model, thereby helping the model reduce computational redundancy and improve model execution efficiency. <br>
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Method</h2>
        <br>
        <div class="columns is-centered has-text-centered">
          <img id="multi_methods" width="80%" src="static/images/multi_methods.png"/>
        </div>
        <!-- <video poster="" id="overview" autoplay muted loop playsinline width="70%">
            <source src="static/videos/intro/overview.mp4"
                    type="video/mp4">
        </video> -->
        <div class="content is-centered has-text-justified">
          <p  style="font-size: 24px;">
            PIVOT-R is a primitive-driven waypoint-aware world model with asynchronous hierarchical executors. 
            It only focuses on the prediction of waypoints related to the manipulation task, and it is easier to predict key nodes in the manipulation task than
other methods. 
            In addition, PIVOT-R sets different execution frequencies for different modules to have higher execution efficiency and lower redundancy.
          </p>
        </div>
        <br>
        <div class="columns is-centered has-text-centered">
            <img id="pipeline" width="80%" src="static/images/pipeline.png"/>
        </div>
        <div class="content is-centered has-text-justified">
          <p  style="font-size: 24px;">
            It mainly consists of a waypoint-aware world model (WAWM) and an action prediction module, 
            where two modules cooperate with each other through an asynchronous hierarchical executor (AHE). 
            In WAWM, we first use pre-trained VLM to perform low-frequency primitive action parsing on user instructions and provide waypoint indications for the scene prediction
            module. 
            Then, the scene prediction module learns to model the world knowledge based on waypoints and manipulation trajectories. 
            Finally, we use a lightweight action prediction module to perform high-frequency action prediction and execution.
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Real Robot Experiments</h2>
        <!-- <img id="robot_result" width="80%" src="static/images/real_robot.png"> -->
        <div class="content has-text-justified">
          <br>
          <p style="font-size: 24px;">
          We conducted real-world experiments, where we set up three tasks: 
          (i) "Pick up": pick up the correct object from the table. 
          (ii) "Put on": Pick up the object and place it on the correct color block. 
          (iii) "Push to": Push the object to the correct color block.  
          We collected 400, 200, and 200 sets of demonstrations respectively. 
          PIVOT-R achieved a 6% improvement over the best baseline.
          </p>
          <div class="columns is-centered has-text-centered">
            <div class="container" style="display: flex; justify-content: center; flex-wrap: nowrap;">
              <div style="flex: 1; margin-right: 2%;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -1%;">
                  <source src="static/videos/push_coffee_to_pink_block.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Push coffee to pink block.</p>
              </div>
              <div style="flex: 1; margin-right: 2%;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -0.5%;">
                  <source src="static/videos/pick_up_the_juice_in the_front_row.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Pick up the juice in the front row.</p>
              </div>
              <div style="flex: 1;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: 0.5%;">
                  <source src="static/videos/pick_up_starbucks_and_put_it_on_yellow_block.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Pick up starbucks and put it on yellow block.</p>
              </div>
            </div>
          </div>
    </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">SeaWave Benchmark Experiments</h2>
        <!-- <img id="calvin_env" width="70%" src="static/images/calvin_env.png" style="display: block; margin-left: auto; margin-right: auto"> -->
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            <br>
             We choose <a href="https://arxiv.org/abs/2306.11335"><b>SeaWave</b></a>, an open-source benchmark to learn multi-level instruction tasks, 
             as our experimental platform, and use the corresponding data as demonstration data for imitation learning. 
             Its greatest advantage is that it provides progressive tasks, facilitating our comprehensive comparison and analysis of the model's capabilities. 
             It supports 8 skills, including daily operations such as grasping and placing objects, opening and closing doors, and more than 3,000 different instructions. 
             The SeaWave dataset covers four different levels of language instructions. 
             We train on this dataset and test on a specially divided test set.  
             Results are shown in the below.
          </p>
          <div class="columns is-centered">
             <img id="table1" width="70%" src="static/images/table1.png" style="display: block; margin-left: auto; margin-right: auto">
          </div>
          <div class="container" style="display: flex; justify-content: space-between; flex-wrap: wrap;">
            <div style="flex: 1; margin: 0.5%; text-align: center;">
              <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -1%;">
                <source src="static/videos/level1.mp4" type="video/mp4">
              </video>
              <p style="font-size: 20px;">Pick up Bernachon.</p>
            </div>
            <div style="flex: 1; margin: 0.5%; text-align: center;">
              <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -0.5%;">
                <source src="static/videos/level2.mp4" type="video/mp4">
              </video>
              <p style="font-size: 20px;">Give me the yogurt.</p>
            </div>
            <div style="flex: 1; margin: 0.5%; text-align: center;">
              <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: 0.5%;">
                <source src="static/videos/level3.mp4" type="video/mp4">
              </video>
              <p style="font-size: 20px;">I'm thirsty, can you use a cup to pour the red-packaged drink for me?</p>
            </div>
            <div style="flex: 1; margin: 0.5%; text-align: center;">
              <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: 0.5%;">
                <source src="static/videos/level4.mp4" type="video/mp4">
              </video>
              <p style="font-size: 20px;">The drink in the lower right corner looks good. Can I have it?</p>
            </div>
          </div>
          <br>
          <div class="columns has-text-centered">
          <p style="font-size: 24px;">
            <br>
            We also perform experiments in different unseen scenarios on level 2, 3, and 4 tasks.
            New scenarios include unseen backgrounds (<i>i.e.</i>, two unseen tables), changing light intensity, and more distractions (<i>i.e.</i>, more objects).
            The results are shown in the below. 
          </p>
          <br>
          </div>
          <div class="columns">
            <img id="table2" width="70%" src="static/images/table2.png" style="display: block; margin-left: auto; margin-right: auto">
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="container" style="display: flex; justify-content: center; flex-wrap: nowrap;">
              <div style="flex: 1; margin-right: 2%;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -1%;">
                  <source src="static/videos/background.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Changed Background</p>
              </div>
              <div style="flex: 1; margin-right: 2%;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -0.5%;">
                  <source src="static/videos/light.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Changed Lights</p>
              </div>
              <div style="flex: 1;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: 0.5%;">
                  <source src="static/videos/multi_objs.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">More Distractors</p>
              </div>
            </div>
          </div>
      </div>
    </div>
</section>

<!-- <section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Ablation Studies</h2>
        <div class="content has-text-justified">
            <div class="column has-text-left" style="padding-top: 25px;">
              <p style="font-size: 24px;">
                GR-1 features video prediction and large-scale pretraining on video prediction.
                We perform ablation studies to study how these two factors influence the performance.
                GR-1 outperforms the variant without pre-training and the variant without pre-training and video prediction in all experiments. 
                We hypothesize that this is because the large-scale video pre-training helps GR-1 learn an accurate video prediction model which helps the robot understand what shall happen in future steps given the language instruction and previous observations. 
                And this information acts as a strong signpost for the robot to generate pertinent actions for rolling out trajectories. 
                Without pre-training, the video prediction of GR-1 w/o Video Pre-training may not be as robust.
                <br><br>
              </p>
            </div>
            <div class="columns">
              <div class="column has-text-left">
                  <img src="static/images/ablation_ABCD_D.png" class="interpolation-image"
                  alt="" style="display: block; margin-left: auto; margin-right: auto"/>
              </div>
              <div class="column has-text-left">
                  <img src="static/images/ablation_ABC_D.png" class="interpolation-image"
                alt="" style="display: block; margin-left: auto; margin-right: auto"/>
              </div>
              <div class="column has-text-left">
                <img src="static/images/ablation_10_percent_data.png" class="interpolation-image"
              alt="" style="display: block; margin-left: auto; margin-right: auto"/>
            </div>
            </div>
            <div class="column has-text-left" style="padding-top: 25px;">
              <p style="font-size: 24px;">
                We probe into GR-1 to investigate its video prediction performance on CALVIN and real robot data.
                Results are shown in the below figure in which the images in the green boxes are the ground-truth images and those in the blue boxes are the predicted images.
                GR-1 is able to reconstruct future frames correctly on both CALVIN data and real robot data, although some details (e.g. occluded objects) are missing. 
                This video prediction signal can serve as a strong guide for action predictions.
                More results can be found in the paper.
              </p>
            </div>
            <img id="robot_result" width="65%" style="display: block;margin-left: auto; margin-right: auto;" src="static/images/fwd_pred.png">
        </div> 
      </div>
    </div>
</section> -->

<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Conclusions</h2>
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            We propose PIVOT-R, a primitive-driven waypoint-aware world model.
            PIVOT-R focuses on the execution of primitive actions. 
            Predicting key waypoints in the future greatly improves performance. 
            It has achieved state-of-the-art results on the SeaWave benchmark, and experiments have proven that it has good robustness. 
            We also use asynchronous hierarchical executors to ensure fast enough execution of the model. 
            In addition, we demonstrate that PIVOT-R has the potential to complete unseen instructions and tasks under the guidance of a high-level VLM. 
            Finally, we also demonstrate PIVOT-R's ability to improve performance through human demonstration. 
            These results illustrate the potential of PIVOT-R.
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title" style="text-align: left;">BibTeX</h2>
        <div class="content has-text-justified">
          <!-- <pre><code style="font-size: 24px;">@misc{wu2023unleashing,
      title={Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation}, 
      author={Hongtao Wu and Ya Jing and Chilam Cheang and Guangzeng Chen and Jiafeng Xu and Xinghang Li and Minghuan Liu and Hang Li and Tao Kong},
      year={2023},
      eprint={2312.13139},
      archivePrefix={arXiv},
      primaryClass={cs.RO}}
          </code></pre> -->
        </div>
      </div>
    </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
        <div class="content">
          <p style="font-size: 24px;">
            The website template was adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
  </div>
</footer>

</body>
</html>
